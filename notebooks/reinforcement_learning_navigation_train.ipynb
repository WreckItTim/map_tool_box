{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d1e01-265d-48e5-b72d-682eb5d0e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from map_tool_box.modules import Environment\n",
    "from map_tool_box.modules import Evaluator\n",
    "from map_tool_box.modules import Rewarder\n",
    "from map_tool_box.modules import Spawner\n",
    "from map_tool_box.modules import Control\n",
    "from map_tool_box.modules import Utils\n",
    "from map_tool_box.modules import Astar\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11d26e-5c33-45eb-b4cf-b5b0aaf9b0d3",
   "metadata": {},
   "source": [
    "# read configuration variables/objects from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474a3cb-8a60-420e-9621-e6afa8194aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = 'config_alpha.py'\n",
    "configs_directory = Utils.get_global('configs_directory')\n",
    "config_in_path = Path(configs_directory, config_name)\n",
    "%run $config_in_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fba4f9-ebb2-42fc-a24f-3f9a156c4ff1",
   "metadata": {},
   "source": [
    "# set additional paramters for DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf67149-990b-4799-b7ac-f216435536db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if restricted on RAM set to true but will run slower\n",
    "memory_saver = True\n",
    "data_map.memory_saver = memory_saver\n",
    "\n",
    "# file IO\n",
    "models_directory = Utils.get_global('models_directory')\n",
    "random_seed = 42\n",
    "project_name = 'AirSim_Navigation'\n",
    "experiment_name = 'DRL_0'\n",
    "trial_name = f'seed_{random_seed}'\n",
    "write_directory = Path(models_directory, project_name, experiment_name, trial_name) # write all results here\n",
    "\n",
    "# rewarder alpha vlaues (scale/weight)\n",
    "goal_distance_alpha = -1 # weight how much progress an action takes towards goal (-) penalizes further away\n",
    "step_alpha = -1 # (-) penalizes each step taken in an episode to encourage shorter paths\n",
    "max_steps_alpha = -10 # (-) gives a large penalty at terminal step if not reached goal\n",
    "goal_alpha = 40 # (+) gives a large reward at terminal step if reached goal\n",
    "\n",
    "# evaluation parameters\n",
    "# values are set extremely low for demonstration purposes -- typically they are much higher (see full in comment)\n",
    "n_eval_paths = 1 # 100 full # number of paths to pull from curriculum dictionaries for each level for evaluations\n",
    "burn_in = 1 # 50_000 full # initial number of episodes before starting to level up cirriculum learning\n",
    "ckpt_freq = 1 # 10_000 full # write neural network weights and other learning loop state vars to file every this many episodes\n",
    "level_up_freq = ckpt_freq # how many episodes until curriclulm level goes to next difficulty\n",
    "eval_frequency = ckpt_freq # evaluate learning curve every eval_frequency episodes\n",
    "max_episodes = 4 # 2_000_000 full # number of training episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a1a21-fe7f-4923-99c8-8c5e8d6ae7e6",
   "metadata": {},
   "source": [
    "# log config.py file for future readin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb168b6-3392-4e52-ad22-906cb053291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_out_path = Path(write_directory, 'config.py')\n",
    "shutil.copy(config_in_path, config_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a75f56-f614-4b3f-8503-fa8fc392d149",
   "metadata": {},
   "source": [
    "# make DRL components and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2acd4-2fa1-4b26-83d1-dbba7df495e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function -- found emperically to perform well\n",
    "rewarders = [\n",
    "    Rewarder.Step(step_alpha),\n",
    "    Rewarder.GoalDistance(goal_distance_alpha),\n",
    "    Rewarder.Goal(goal_alpha, goal_tolerance),\n",
    "    Rewarder.MaxSteps(max_steps_alpha, steps_multiplier),\n",
    "]\n",
    "\n",
    "# curriculum learning paths -- astar difficulties split into train/val/test\n",
    "# this will read all paths and spawn randomly during training\n",
    "train_paths_train = Astar.read_curriculum(map_name, astar_version, 'train')\n",
    "# this will read only n_eval_paths and spawn iteratively and exhaustively during evaluations\n",
    "train_paths_eval = Astar.read_curriculum(map_name, astar_version, 'train', n_paths=n_eval_paths) # TODO add difficulties\n",
    "val_paths_eval = Astar.read_curriculum(map_name, astar_version, 'val', n_paths=n_eval_paths)\n",
    "test_paths_eval = Astar.read_curriculum(map_name, astar_version, 'test', n_paths=n_eval_paths)\n",
    "\n",
    "# spawner -- decides how set starting and target positions on map\n",
    "train_spawner_train = Spawner.CurricululmTrain(train_paths_train)\n",
    "train_spawner_eval = Spawner.CurricululmEval(train_paths_eval)\n",
    "val_spawner_eval = Spawner.CurricululmEval(val_paths_eval)\n",
    "test_spawner_eval = Spawner.CurricululmEval(test_paths_eval)\n",
    "\n",
    "# place holder (for SB3 we need to make environment before making model)\n",
    "model = None\n",
    "\n",
    "# evaluation methods -- do we do anything after each episode/step?\n",
    "evaluation_environments = {\n",
    "    'train':Environment.Episodic(data_map, train_spawner_eval, actor, observer, terminators),\n",
    "    'val':Environment.Episodic(data_map, val_spawner_eval, actor, observer, terminators),\n",
    "    'test':Environment.Episodic(data_map, test_spawner_eval, actor, observer, terminators),\n",
    "}\n",
    "difficulties = train_spawner_train.difficulties\n",
    "evaluators = [\n",
    "    Evaluator.Curriculum(difficulties, level_up_freq, burn_in), # levels up curriculum difficulty every n-episodes\n",
    "    Evaluator.LearningCurve(evaluation_environments, model, eval_frequency, write_directory),  # evaluates on path sets every n-episodes\n",
    "    Evaluator.MaxEpisodes(max_episodes),\n",
    "]\n",
    "\n",
    "# training environment -- primary source of everything RL (how do we step through an episode?)\n",
    "starting_difficulty = difficulties[0]\n",
    "train_environment = Environment.ReinforcementLearning(data_map, train_spawner_train, actor, observer, rewarders, terminators, \n",
    "                                                evaluators, starting_difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b34710-2a6d-4d30-babf-15a22ee9dc43",
   "metadata": {},
   "source": [
    "# make neural network action model (StableBaselines3 for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517abc94-4dee-471e-91de-6838a0f4d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch_layers = 3\n",
    "net_arch_nodes = 64\n",
    "total_policy_scale = 2\n",
    "buffer_size = 1_000 # 100_000 full -- takes <50 gb memory\n",
    "gamma = 0.99\n",
    "device = 'cuda:0'\n",
    "total_timesteps = 1_000_000_000 # we overwite the trigger for end of learning with episodes not steps\n",
    "exploration_fraction = 0.1\n",
    "\n",
    "from map_tool_box.modules import SB3Wrapper\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import DQN as sb3Class\n",
    "\n",
    "# sb3 wrappers\n",
    "observer_sb3 = SB3Wrapper.SB3Observer(observer)\n",
    "actor_sb3 = SB3Wrapper.SB3Actor(actor)\n",
    "train_environment_sb3 = SB3Wrapper.SB3Environment(train_environment, observer_sb3, actor_sb3)\n",
    "\n",
    "print('SB3 gym observation space:', train_environment_sb3.observation_space)\n",
    "print('SB3 gym action space:', train_environment_sb3.action_space)\n",
    "\n",
    "sb3_model_arguments = {\n",
    "    'policy':'MultiInputPolicy',\n",
    "    'env':train_environment_sb3,\n",
    "    'buffer_size':buffer_size,\n",
    "    'gamma':gamma,\n",
    "    'device':device,\n",
    "    'exploration_fraction':exploration_fraction,\n",
    "    'policy_kwargs':{\n",
    "        'net_arch':[int(total_policy_scale*net_arch_nodes) for _ in range(net_arch_layers)],\n",
    "    },\n",
    "}\n",
    "sb3_model = sb3Class(**sb3_model_arguments)\n",
    "model = SB3Wrapper.ModelSB3(sb3_model)\n",
    "\n",
    "sb3_model.q_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361022f2-3263-4925-b6a0-683d4aba5255",
   "metadata": {},
   "source": [
    "# now set the model placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960c3e-64a0-4e03-b4ac-7df4c4212cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this evaluation component to have the model to play episodes\n",
    "# this is hardcoded for the moment TODO improve this\n",
    "evaluators[1].model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129ce66-407f-41a5-b19d-2226155a4239",
   "metadata": {},
   "source": [
    "# step through one episode of envrionment with untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37610f5-76db-48ff-a042-11451fa43f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which environment to evaluate\n",
    "eval_environment = train_environment\n",
    "\n",
    "# run through episode (view map_tool_box.modules.Environment.py to see what is saved in states)\n",
    "states = Control.play_episode(eval_environment, model, save_states=True, save_observations=True)\n",
    "\n",
    "# make animation from episodic states\n",
    "animation = eval_environment.animate_episode(states)\n",
    "\n",
    "# show animation in notebook\n",
    "display(HTML(animation.to_jshtml()))\n",
    "\n",
    "# save as gif\n",
    "#animation.save('episode.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ecd4fc-86ce-4795-b09c-479588ef4d16",
   "metadata": {},
   "source": [
    "# Train RL Model (StableBaselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca0df6-4762-4401-bc13-3c8fb2070320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is commented out for now -- just use the random initialized weights\n",
    "# full training takes a day or two to run (this is an example notebook with the values dialed way back to finish in a few minutes)\n",
    "\n",
    "# allow us to control when to terminate training from SB3 learn()\n",
    "class StopLearning(BaseCallback):\n",
    "    def __init__(self, train_environment_sb3, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.environment = train_environment_sb3\n",
    "    def _on_step(self) -> bool:\n",
    "        return self.environment.continue_learning\n",
    "\n",
    "# call sb3 learn method\n",
    "callback = StopLearning(train_environment)\n",
    "train_environment.start_learning()\n",
    "sb3_model.learn(\n",
    "    total_timesteps,\n",
    "    callback = callback,\n",
    "    log_interval = -1,\n",
    "    tb_log_name = None,\n",
    "    reset_num_timesteps = False,\n",
    ")\n",
    "\n",
    "# save to file\n",
    "model_path = Path(write_directory, 'model.zip')\n",
    "sb3_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac5d88-600e-4720-a986-4a534bb312bc",
   "metadata": {},
   "source": [
    "# cheat and set learned weights from a previous training run (took 2 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4ac58-1700-40dc-8edb-5962e8945727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "weights_path = Path(models_directory, project_name, 'learned_weights.pt') # write all results here\n",
    "state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "sb3_model.q_net.load_state_dict(state_dict)\n",
    "sb3_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b786eb-a395-48ab-9231-08877d41913d",
   "metadata": {},
   "source": [
    "# view learning curve from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be65479-3126-429b-b1de-6d75fb720889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this learning curve is a failed curve from a trial run\n",
    "curve_path = Path(models_directory, project_name, experiment_name, trial_name, 'learning_curve.p')\n",
    "learning_curve = Utils.pickle_read(curve_path)\n",
    "for key in learning_curve:\n",
    "    plt.plot(np.array(learning_curve[key])*100, label=key)\n",
    "plt.legend()\n",
    "plt.xlabel(f'DRL checkpiont every {ckpt_freq} episodes')\n",
    "plt.ylabel(f'navigation accuracy [%]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30124225-8bda-486b-b262-b0c2c0ad839c",
   "metadata": {},
   "source": [
    "# step through an episode with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc68002-1fab-4075-82db-97d229acf89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_environments['test'].spawner.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba18c4-8fd8-41b3-b3d6-a213fa7cb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which environment to evaluate\n",
    "eval_environment = evaluation_environments['test']\n",
    "\n",
    "# run through episode (view map_tool_box.modules.Environment.py to see what is saved in states)\n",
    "states = Control.play_episode(eval_environment, model, save_states=True, save_observations=True)\n",
    "\n",
    "# make animation from episodic states\n",
    "animation = eval_environment.animate_episode(states)\n",
    "\n",
    "# show animation in notebook\n",
    "display(HTML(animation.to_jshtml()))\n",
    "\n",
    "# save as gif\n",
    "#animation.save('episode.gif', writer='pillow', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ada7b-d7a2-455d-aa32-ed7be0aa0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb3_model.q_net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216ce8d-8a42-45b7-a80d-c414b04c6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from map_tool_box.modules import Model\n",
    "model2 = Model.read_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f202141-5c91-4938-b665-93660c93aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.q_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
